{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import pyranges as pr\n",
    "import lotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Find candidate peaks')\n",
    "\n",
    "# parser.add_argument('file', help='bigwig file')\n",
    "# parser.add_argument('-t', '--threshold-list', nargs='*', help='values multiplied by background to set threshold for candidate peaks| DEFAULT: 2 4 6', default=[2, 4, 6])\n",
    "# parser.add_argument('-b', '--background_list', nargs='*', help='values to set size of region for used for background coverage| DEFAULT: 10000 100000 1000000', default=[10000, 100000, 1000000])\n",
    "# parser.add_argument('-w', '--window-list', nargs='*', help='window size in basepairs used to smooth bigwig file| DEFAULT: 200 400 600', default=[200, 400, 600])\n",
    "# parser.add_argument('-c', '--threshold-cumulative', type=int, help='number of different enrichment tests passed by region to be called a candidate peak; max = threshold-list count * background-list count * window-list count| DEFAULT: 9', default=9)\n",
    "# parser.add_argument('-n', '--min', type=int, help='minimum peak size| DEFAULT: 50', default=50)\n",
    "# parser.add_argument('-x', '--max', type=int, help='maximum peak size| DEFAULT: 500', default=500)\n",
    "# parser.add_argument('-f', '--folder', type=str, help='folder to write results to| DEFAULT: ./', default='./')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# bw_file = args.file\n",
    "# threshold_list = args.threshold_list\n",
    "# threshold_list = [float(i) for i in threshold_list]\n",
    "# background_list = args.background_list\n",
    "# background_list = [int(i) for i in background_list]\n",
    "# window_list = args.window_list\n",
    "# window_list = [int(i) for i in window_list]\n",
    "# threshold_cumulative_init = args.threshold_cumulative\n",
    "# threshold_cumulative_init = int(threshold_cumulative_init)\n",
    "# min_size = args.min\n",
    "# min_size = int(min_size)\n",
    "# max_size = args.max\n",
    "# max_size = int(max_size)\n",
    "# folder = args.folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_file = '/project/hugheslab/datashare/lhentges/biobank/atac/Don001_ATAC_d13.hg38.bw'\n",
    "threshold_list = [2, 4, 6]\n",
    "background_list = [10**4, 10**5, 10**6]\n",
    "window_list = [200, 400, 600]\n",
    "threshold_cumulative_init = 9\n",
    "min_size = 50\n",
    "max_size = 500\n",
    "folder = '/home/l/lhentges/code/repos/LoTron2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom_list = []\n",
    "pybw_object = pyBigWig.open(bw_file)\n",
    "for chrom in pybw_object.chroms():\n",
    "    if (not chrom.startswith('chrUn')) and ('_' not in chrom) and (chrom != 'chrM') and (chrom != 'chrEBV'):\n",
    "        chrom_list.append(chrom)\n",
    "pybw_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_data = lotron2.BigwigData(bw_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1\n",
      "chr2\n",
      "chr3\n",
      "chr4\n",
      "chr5\n",
      "chr6\n",
      "chr7\n",
      "chr8\n",
      "chr9\n",
      "chr10\n",
      "chr11\n",
      "chr12\n",
      "chr13\n",
      "chr14\n",
      "chr15\n",
      "chr16\n",
      "chr17\n",
      "chr18\n",
      "chr19\n",
      "chr20\n",
      "chr21\n",
      "chr22\n",
      "chrX\n",
      "chrY\n"
     ]
    }
   ],
   "source": [
    "threshold_limit = len(background_list)*len(window_list)*len(threshold_list)\n",
    "total_df = pd.DataFrame(columns=['Chromosome', 'Start', 'End'])\n",
    "\n",
    "for chrom in chrom_list:\n",
    "    print(chrom)\n",
    "    \n",
    "    threshold_cumulative = threshold_cumulative_init\n",
    "    chrom_coverage_array, chrom_stats_dict = bw_data.get_chrom_info_make_coverage_map(chrom, return_chrom_stats_dict=True)\n",
    "    background_global_min = chrom_stats_dict['chrom_mean']\n",
    "    enriched_regions, threshold_array = lotron2.find_enriched_regions_param_grid(chrom_coverage_array, background_list, window_list, threshold_list, threshold_cumulative, background_global_min=background_global_min, return_threshold_array=True)\n",
    "\n",
    "\n",
    "    enriched_regions_df = pd.DataFrame(enriched_regions, columns=['Start', 'End'])\n",
    "    enriched_regions_df.insert(0, 'Chromosome', chrom)\n",
    "\n",
    "    enriched_regions_df['initial_size'] = enriched_regions_df.End - enriched_regions_df.Start\n",
    "    enriched_regions_df = enriched_regions_df.drop(enriched_regions_df[enriched_regions_df['initial_size']<min_size].index)\n",
    "    solved_df = enriched_regions_df[enriched_regions_df['initial_size']<=max_size]\n",
    "    unsolved_df = enriched_regions_df[enriched_regions_df['initial_size']>max_size]\n",
    "\n",
    "    solved_df = solved_df[['Chromosome', 'Start', 'End']]\n",
    "    unsolved_df = unsolved_df[['Chromosome', 'Start', 'End']]\n",
    "\n",
    "    while (not unsolved_df.empty) and (threshold_cumulative < threshold_limit-1):\n",
    "\n",
    "        # sort unsolved_df by Start\n",
    "        unsolved_df = unsolved_df.sort_values(['Chromosome', 'Start'])\n",
    "        unsolved_df.reset_index(drop=True, inplace=True)\n",
    "        unsolved_df['unsolved_idx'] = unsolved_df.index\n",
    "        \n",
    "\n",
    "        # rerun test with higher threshold\n",
    "        threshold_cumulative += 1\n",
    "        enriched_regions_higher_thresh = lotron2.find_enriched_regions(threshold_array, threshold_cumulative)\n",
    "        enriched_regions_higher_thresh_df = pd.DataFrame(enriched_regions_higher_thresh, columns=['Start', 'End'])\n",
    "        enriched_regions_higher_thresh_df.insert(0, 'Chromosome', chrom)\n",
    "\n",
    "\n",
    "        # use pyranges to find overlaps between unsolved regions and enriched regions at a higher threshold\n",
    "        unsolved_pr = pr.PyRanges(unsolved_df)\n",
    "        enriched_regions_higher_thresh_pr = pr.PyRanges(enriched_regions_higher_thresh_df)\n",
    "        prs_for_overlap_dict = {k: v for k, v in zip(['enriched initial', 'enriched higher thresh'], [unsolved_pr, enriched_regions_higher_thresh_pr])}\n",
    "        overlap_counts_df = pr.count_overlaps(prs_for_overlap_dict).as_df()\n",
    "        overlap_counts_df['size'] = overlap_counts_df.End - overlap_counts_df.Start\n",
    "        \n",
    "\n",
    "        # find regions that are enriched initially, but not enriched at any coordinates with higher threshold\n",
    "        single_enriched_df = overlap_counts_df[(overlap_counts_df['enriched initial'] == 1) & (overlap_counts_df['enriched higher thresh'] == 0)]\n",
    "        single_enriched_df = pd.merge(single_enriched_df, unsolved_df, on=['Chromosome', 'Start', 'End'], how='inner', suffixes=('_single_enriched', '_unsolved'))\n",
    "        solved_df = pd.concat([solved_df, unsolved_df.loc[single_enriched_df['unsolved_idx']]], join='inner')\n",
    "        unsolved_df.drop(single_enriched_df['unsolved_idx'], inplace=True)\n",
    "\n",
    "\n",
    "        # find regions that are both enriched initially and enriched at coordinates with higher threshold\n",
    "        double_enriched_df = overlap_counts_df[(overlap_counts_df['enriched initial'] == 1) & (overlap_counts_df['enriched higher thresh'] == 1)]\n",
    "        double_enriched_df = pd.merge_asof(double_enriched_df, unsolved_df, on='Start', by='Chromosome', suffixes=('_double_enriched', '_unsolved'))\n",
    "        double_enriched_df['End'] = double_enriched_df['End_double_enriched']\n",
    "        double_enriched_df['below_max_size'] = double_enriched_df['size'] < max_size\n",
    "\n",
    "\n",
    "        solved_df = pd.concat([solved_df, double_enriched_df[double_enriched_df['below_max_size']]], join='inner')\n",
    "        unsolved_df.drop(double_enriched_df[double_enriched_df['below_max_size']]['unsolved_idx'].unique(), inplace=True)\n",
    "        unsolved_df = unsolved_df[['Chromosome', 'Start', 'End']]\n",
    "\n",
    "\n",
    "    solved_df = pd.concat([solved_df, unsolved_df])\n",
    "    solved_df = solved_df[['Chromosome', 'Start', 'End']]\n",
    "    total_df = pd.concat([total_df, solved_df])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.sort_values(['Chromosome', 'Start'])\n",
    "total_df.to_csv(folder+'candidate_peak_beta.bed', index=False, sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
